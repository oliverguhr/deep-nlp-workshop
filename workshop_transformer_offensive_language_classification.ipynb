{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "workshop-transformer-offensive-language-classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliverguhr/deep-nlp-workshop/blob/main/workshop_transformer_offensive_language_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x7ywAcjTMyk"
      },
      "source": [
        "# Offensive Language Classification\n",
        "\n",
        "\n",
        "## Vorbereitung\n",
        "\n",
        "Zurerst laden wir uns die notwendigen Daten und Pakete herrunter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6cFhLiDTMyk"
      },
      "source": [
        "!pip install datasets transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqhVeVEnTMyl"
      },
      "source": [
        "!wget -c https://www.htw-dresden.de/~guhr/dist/sample/germeval2018.training.txt\n",
        "!wget -c https://www.htw-dresden.de/~guhr/dist/sample/germeval2018.test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONd9nMwMTMyl"
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wCOi_UiTMyl"
      },
      "source": [
        "## Vorbereiten der Daten\n",
        "\n",
        "Im nächsten Schritt müssen wir die Daten laden und etwas anpassen. Die Daten liegen Tabstopp getrennte csv vor. Für die einfache Verarbeitung bietet sich Pandas an, könnte aber auch mit Python boardmitteln erledigt werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpvStxVhTMyl"
      },
      "source": [
        "test_df = pd.read_csv(\"germeval2018.test.txt\", sep='\\t', header=0,encoding=\"utf-8\")\n",
        "train_df = pd.read_csv(\"germeval2018.training.txt\", sep='\\t', header=0,encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9CkOlvaTMyl"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BsXBCsTMyn"
      },
      "source": [
        "# Die nicht benötigten Spalten werden gelöscht\n",
        "test_df.drop(columns=['label2'], inplace=True)\n",
        "train_df.drop(columns=['label2'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4P20Gf_TMyn"
      },
      "source": [
        "def clean_text (text):\n",
        "    #text = text.str.lower() # lowercase\n",
        "    #text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
        "    #text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
        "    #text = text.str.replace(r\"@\",\"\")\n",
        "    #text = text.str.replace(r\"[^A-Za-z0-9öäüÖÄÜß()!?]\", \" \")\n",
        "    #text = text.str.replace(\"\\s{2,}\", \" \")\n",
        "    return text\n",
        "\n",
        "def convert_label(label):\n",
        "    return 1 if label == \"OFFENSE\" else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p690qluXTMyn"
      },
      "source": [
        "train_df[\"text\"]=clean_text(train_df[\"text\"])\n",
        "test_df[\"text\"]=clean_text(test_df[\"text\"])\n",
        "train_df[\"label\"]=train_df[\"label\"].map(convert_label)\n",
        "test_df[\"label\"]=test_df[\"label\"].map(convert_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BIixoz-TMyn"
      },
      "source": [
        "# und jetzt sehen unsere Daten so aus\n",
        "train_df.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeE1qHXhTMyo"
      },
      "source": [
        "len(train_df.loc[train_df[\"label\"]==1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1_tIVzLTMyo"
      },
      "source": [
        "Wieviele Datensätze haben wir in unserem Train/Valid/Test sets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rCCzWaJTMyo"
      },
      "source": [
        "print(f\"Test exampels \\t {len(test_df) }\")\n",
        "print(f\"Train exampels \\t {len(train_df[500:])}\")\n",
        "print(f\"Valid exampels \\t {len(train_df[:500])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zRn0t3oTMyp"
      },
      "source": [
        "# Im letzten schritt convertieren wir unsere Daten in ein Format, welches die ML Bibliothek nutzen kann.\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df[500:])\n",
        "valid_dataset = Dataset.from_pandas(train_df[:500])\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qt9p2yeTMyp"
      },
      "source": [
        "# Wie sieht unser dataset jetzt aus?\n",
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUhmAob8TMyp"
      },
      "source": [
        "## Encoding der Daten\n",
        "\n",
        "Wir wandeln unsere Texte mit dem Tokenizer des Models in Vekoren um, die unser Model verarbeiten kann. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxDv4WeXTMyp"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "\n",
        "# was passiert wenn wir \"distilbert-base-multilingual-cased\" nutzen?\n",
        "\n",
        "model_checkpoint =\"distilbert-base-multilingual-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkNiW-kITMyp"
      },
      "source": [
        "demo_tokens = tokenizer([\"Mehr Daten führen oftmals zu besseren Ergebnissen.\", \"And this is a second sentence\"], truncation=True)\n",
        "demo_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjSb3j1RTMyp"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(demo_tokens['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CBg2qhVTMyp"
      },
      "source": [
        "def data_tokenizer(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True,padding=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWghamclTMyp"
      },
      "source": [
        "encoded_train_dataset = train_dataset.map(data_tokenizer, batched=True)\n",
        "encoded_valid_dataset = valid_dataset.map(data_tokenizer, batched=True)\n",
        "encoded_test_dataset = test_dataset.map(data_tokenizer, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0oLsPndz1vS"
      },
      "source": [
        "encoded_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhkxSIVLTMyq"
      },
      "source": [
        "## Das Training \\o/\n",
        "\n",
        "Nun können wir unser Model trainieren. Dazu müssen wir noch eine Reihe von Einstellungen (Hyperparameter) festlegen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL6Y-LsbTMyq"
      },
      "source": [
        "metric = load_metric('f1')\n",
        "\n",
        "def compute_metrics(eval_pred):    \n",
        "    predictions, labels = eval_pred    \n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4DtlapiTMyq"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"test-offsive-language\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy =\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    weight_decay=0.001,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\", \n",
        "    fp16=True   \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0v5GJXmTMyq"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_train_dataset,\n",
        "    eval_dataset=encoded_valid_dataset,    \n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_lUSvpDTMyq"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i98JNibNTMyr"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfEjiV_p7BRh"
      },
      "source": [
        "# Wieviel GPU Speicher haben wir belegt? \n",
        "!nvidia-smi\n",
        "#import torch\n",
        "#torch.cuda.empty_cache()\n",
        "#!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w-nuYI8TMyr"
      },
      "source": [
        "#tensorboard --logdir runs\n",
        "#%load_ext tensorboard\n",
        "#%reload_ext tensorboard\n",
        "%tensorboard --logdir ./test-offsive-language/runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-PghAYTMyr"
      },
      "source": [
        "## Tests\n",
        "\n",
        "Im nächsten Schritt können wir testen wir unser Modell mit den bereitgestellten Testdaten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYI9LEbvTMyr"
      },
      "source": [
        "result = trainer.predict(encoded_test_dataset)\n",
        "result.metrics[\"test_f1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiYw4kS3TMyr"
      },
      "source": [
        "import torch\n",
        "\n",
        "#trainer.prediction_step(trainer.model,tokenizer(\"das ist ein test\"),False)\n",
        "trainer.model.cpu()\n",
        "#trainer.model.num_parameters()\n",
        "encoded_texts = tokenizer([\"du bist so dumm\", \"du bist toll\"],padding=True, return_tensors=\"pt\")\n",
        "print(encoded_texts)\n",
        "logits = trainer.model(**encoded_texts)\n",
        "probabilities = torch.softmax(logits[0],dim=1)\n",
        "print(probabilities)\n",
        "class_label = torch.argmax(probabilities,dim=1)\n",
        "print(class_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eyM790HTMyr"
      },
      "source": [
        "# Wie lange dauert es eine einzelne Vorhersage zu berechnen?\n",
        "\n",
        "def predict(text):\n",
        "    trainer.model.cpu()\n",
        "    #trainer.model.num_parameters()\n",
        "    encoded_texts = tokenizer(text, return_tensors=\"pt\")\n",
        "    #print(encoded_texts)\n",
        "    logits = trainer.model(**encoded_texts)\n",
        "    probabilities = torch.softmax(logits[0],dim=1)\n",
        "    #print(probabilities)\n",
        "    class_label = torch.argmax(probabilities)\n",
        "    return class_label\n",
        "    #print(class_label)\n",
        "\n",
        "%timeit predict(\"du bist so toll\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsL5FUdTTMys"
      },
      "source": [
        "# Jetzt bist du drann :)\n",
        "\n",
        "Unsere Ergebnisse sind schon ganz gut - aber wir können die Ergebisse noch verbessern. \n",
        "Mache dich zuerst mit dem Notebook vertraut - ändere ein paar Parameter wie Lernrate und die Anzahl der Epochen und schau dir an wie diese das Ergebiss verändern. \n",
        "\n",
        "Hier sind bei paar Ideen für deine Experimente:\n",
        "\n",
        "* Wir haben das Modell erst eine Epoch trainiert. Was passtiert wenn wir es zwei oder drei Epochen trainieren?\n",
        "\n",
        "* Teste verschiedene Modelle aus. Der [Model Hub](https://huggingface.co/models) listet eine Reihe von deutschen modellen mit denen du die Ergebnisse verbessern kannst. \n",
        "\n",
        "* Rund 5000 Datensätze sind vergleichweise wenig für dieses Problem. Evtl. findest du weitere Datensätze die du zum aktuellen Trainingsdatensatz hinzufügen kannst.\n",
        "\n",
        "* Im [Model Hub](https://huggingface.co/models) stehen eine Reihe von multilingualen Modellen zur Verfügung. Diese Modelle wurden mit verschiedenen Sprachen trainiert. Du könntest ebenfalls versuchen den deutschen Datensatz einen englischen hinzuzufügen um ein mehrsprachiges Modell zu trainieren. Möglicherweise ist dieses auch auf den deutschen Daten besser. \n",
        "\n",
        "Data Augmentation ist ein Verfahren um neue Datensätze zu erzeugen, in dem man bestehende Datensätze etwas modifiziert. Wichtig ist dabei, das sich die Aussage nicht ändert (die Klasse die gleiche bleibt)\n",
        "\n",
        "* Du kannst Synonyme Wörter ersetzen und so neue Datensätze generieren. Ein Beispiel:\n",
        "\n",
        "> \"Kann man diesen ganzen Scheiß noch glauben?\" -> \"Kann man diesen ganzen Mist noch glauben?\"\n",
        "\n",
        "* Alles ist hier erlaubt. Versuche Texte von Deutsch nach Englisch und wieder nach Deutsch zu übersetzen. Wenn der Sinn erhalten kann das Ergebniss auch für das Training genutzt werden. Ein kleines Beispiel mit Google Translate:\n",
        "\n",
        "> Deutsch: \"Kann man diesen ganzen Scheiß noch glauben?\" \n",
        "\n",
        "> Englisch: \"Can you still believe all this shit?\"\n",
        "\n",
        "> Deutsch: \"Kannst du all diese Scheiße noch glauben?\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDO2hHOI14XU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}